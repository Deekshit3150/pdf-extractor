
    <html>
    <head>
        <title>Extracted PDF Content</title>
        <style>
            body { font-family: Arial; padding: 20px; background: #f9f9f9; }
            h2 { border-bottom: 2px solid #ccc; margin-top: 40px; }
            .page-block { padding: 15px; background: #fff; margin-bottom: 20px; border-radius: 8px; box-shadow: 0 0 5px rgba(0,0,0,0.1); }
            table { border-collapse: collapse; width: 100%; margin-top: 10px; }
            table, th, td { border: 1px solid #aaa; padding: 5px; }
            img { max-width: 300px; margin: 10px 5px; }
        </style>
    </head>
    <body>
        <h1>Extracted PDF Content</h1>
    <div class="page-block"><h2>Page 1</h2><h3>Text</h3><p>AI/ML-based Real-Time Sign Language Converter: Enabling Seamless Communication via<br>Audio Calls for Deaf and Mute Individuals<br>Dr. Prasanthi Rathnala, Dr. P Naresh*, Gogineni Deekshit, Dadi Naga seshu, I Rusheeta, P<br>Megana; *npatnana@gitam.edu<br>GITAM deemed to be university, visakhapatanam.<br>Abstract<br>The increasing adoption of Artificial Intelligence (AI) and Machine Learning (ML) technologies<br>has created new possibilities in improving accessibility and communication for people with<br>disabilities. One of the most significant challenges faced by the deaf and mute communities<br>is the communication barrier with non-sign language users. This paper proposes an AI/ML-<br>based real-time sign language converter that enables seamless communication via audio calls<br>for deaf and mute individuals. The proposed system integrates multiple components,<br>including gesture recognition technology and computer vision, to translate sign language<br>gestures into text and audio. The real-time converter uses machine learning models to<br>recognize hand and facial gestures, then converts the recognized gestures into text, which is<br>subsequently converted into speech via a text-to-speech (TTS) engine. The system also<br>ensures minimal latency, enhancing the user experience during live interactions. Experimental<br>results show that the system offers an efficient, real-time solution for overcoming<br>communication barriers, fostering inclusivity and independence for deaf and mute individuals.<br>Keywords: AI, Machine Learning, Real-time communication, Sign Language Recognition,<br>Gesture recognition, Text-to-Speech, Deaf and mute communication<br>1. Introduction<br>Communication is a fundamental aspect of human interaction, and for individuals who are<br>deaf or mute, traditional communication methods such as speech are inaccessible. In recent<br>years, advancements in Artificial Intelligence (AI) and Machine Learning (ML) have opened up<br>new possibilities for bridging this gap. Sign language, as the primary mode of communication<br>for many deaf and mute individuals, remains a challenge in communication with those who<br>do not know it. While existing technologies have provided solutions like text-based<br>communication apps and interpreters, they fall short in real-time interactions, particularly in<br>scenarios like audio calls [1]. The need for interpreters or the switching between modes of<br>communication can lead to delays, reducing the fluidity of conversations [2,3]. To address this<br>challenge, this paper proposes a novel AI/ML-based Real-Time Sign Language Converter<br>designed to translate sign language gestures into text and audio in real time, thus enabling<br>seamless communication via audio calls for deaf and mute individuals.<br>The quest for effective communication tools for deaf and non-verbal individuals has led to the<br>development of various assistive technologies, but many existing solutions still fall short,<br>particularly in enabling real-time communication. Text-based systems, such as apps that<br>transcribe speech or convert sign language to text, offer benefits but face challenges like</p><h3>Links</h3><p>[No links found]</p><h3>Tables</h3><p>[No tables found]</p><h3>Images</h3><p>[No images found]</p></div><div class="page-block"><h2>Page 2</h2><h3>Text</h3><p>communication delays, limited interactivity, and restricted accessibility [8]. Vision-based<br>systems that use video feeds to recognize gestures also struggle with environmental<br>challenges like poor lighting or cluttered backgrounds and fail to capture non-manual signals<br>such as facial expressions, head movements, and body posture, which are crucial for full sign<br>language communication [9]. Additionally, these systems often lack the real-time<br>performance necessary for seamless communication. Sensor-based technologies, such as<br>motion-sensing gloves, enhance accuracy by focusing on manual gestures but neglect other<br>elements like facial expressions and have issues with user comfort, intrusiveness, and<br>scalability [10]. Multi-camera systems, while improving accuracy by capturing gestures from<br>multiple angles, are bulkier, more complex, and costly, making them unsuitable for casual or<br>individual use [4,5]. To overcome these limitations, hybrid solutions integrating vision, sensor,<br>and AI approaches are emerging, such as multi-modal systems that incorporate hand gestures,<br>facial expressions, and body posture for more accurate translations, and edge computing for<br>real-time recognition on mobile devices [6-7]. Despite significant progress, creating a<br>seamless, real-time, and contextually aware sign language recognition system remains a<br>challenge due to issues like accuracy, environmental factors, real-time performance, and user<br>comfort.<br>The paper is structured as follows: Section 2 discusses the methodology behind the real-time<br>sign language converter, including the AI/ML models employed for gesture recognition, data<br>preprocessing, and text-to-speech conversion. Section 3 provides details on the system<br>architecture and implementation. Section 4 presents experimental results, highlighting the<br>system's performance in real-world scenarios. Section 5 concludes with the potential<br>applications of the system and future work.<br>2. Methodology<br>The system consists of three key modules: Gesture Recognition, Text Conversion, and Audio<br>Output.<br>Gesture Recognition Module: This module processes the raw data captured by<br>cameras or sensors, utilizing deep learning models (CNN, LSTM) and computer vision<br>algorithms (OpenCV, MediaPipe) for hand and facial gesture detection.</p><h3>Links</h3><p>[No links found]</p><h3>Tables</h3><p>[No tables found]</p><h3>Images</h3><img src="extracted_images\page2_0.png" alt="Image from Page 2"></div><div class="page-block"><h2>Page 3</h2><h3>Text</h3><p>Text Conversion Module: After recognizing the gestures, this module translates them<br>into text. It uses a sequence-to-sequence model, along with an Attention mechanism,<br>to ensure accurate contextual translation.<br>Audio Output Module: The final step involves converting the recognized text into<br>speech using a TTS engine. The audio is then transmitted to the other party in the<br>conversation.<br>2.1 AI/ML-Based Gesture Recognition<br>The core of the proposed system is its ability to recognize and translate sign language gestures<br>into text. The system employs deep learning algorithms for real-time gesture recognition.<br>Specifically, Convolutional Neural Networks (CNNs) are used for recognizing hand shapes and<br>movements, while Recurrent Neural Networks (RNNs) or Long Short-Term Memory (LSTM)<br>networks capture temporal dependencies between gestures, ensuring that sequential<br>gestures are interpreted correctly.<br>To ensure accuracy and contextual relevance, the system integrates an Attention mechanism.<br>This allows the model to focus on specific parts of the input (such as hand movements, facial<br>expressions, or body posture) that carry the most relevant information. The Attention<br>mechanism helps the system determine the most important gestures for translation, reducing<br>noise from irrelevant data.<br>2.2 Data Preprocessing and Gesture Segmentation<br>Before the gesture recognition models can process the raw input data, the captured images<br>or videos undergo several preprocessing steps. These steps involve:<br>1. Gesture Recognition: Using computer vision techniques, such as OpenCV and<br>MediaPipe, the system detects and isolates the hands and face in each frame,<br>removing any background noise that could interfere with recognition.<br>2. Gesture Detection: After detecting the hand or face, the algorithm calculates the<br>distance between key landmarks, such as the thumb tip and index finger tip, to classify<br>specific gestures. For instance, a large distance between the thumb and index finger<br>can indicate an "open hand" gesture, while a smaller distance or the closure of<br>multiple fingers can be used to identify a "closed fist" gesture. By comparing these<br>distances or relative angles between various hand landmarks, the system can classify<br>and recognize different hand gestures in real-time. The detailed procedure of the<br>detection process used is mentioned in Algorithm 1.</p><h3>Links</h3><p>[No links found]</p><h3>Tables</h3><p>[No tables found]</p><h3>Images</h3><p>[No images found]</p></div><div class="page-block"><h2>Page 4</h2><h3>Text</h3><p>Algorithm 1:<br>--------------------------------------------------------------------------------------------------------------------------<br>1. Import required libraries: OpenCV for image processing, MediaPipe for hand and face<br>landmark detection.<br>2. Initialize video capture object for real-time camera input using cv2.VideoCapture(0).<br>3. Initialize MediaPipe Hand and Face Detection models:<br>o Configure the mp.solutions.hands.Hands() model for hand detection.<br>o Configure the mp.solutions.face_detection.FaceDetection() model for face<br>detection.<br>4. Capture frames from the webcam using the cv2.VideoCapture() object.<br>5. Convert each captured frame from BGR to RGB using cv2.cvtColor(frame,<br>cv2.COLOR_BGR2RGB).<br>6. Pass the RGB frame to MediaPipe Hand and Face Detection models:<br>o Detect hands by calling hands.process(rgb_frame).<br>o Detect faces by calling face_detection.process(rgb_frame).<br>7. Initialize a blank mask with the same dimensions as the frame using<br>np.zeros_like(frame).<br>8. For each detected hand, draw a mask around the hand using the landmarks from the<br>hands.process() output (either as bounding boxes or polygons).<br>9. For each detected face, draw a rectangle in the mask using the bounding box<br>coordinates from the face_detection.process() output.<br>10. Apply the mask to the original frame using cv2.bitwise_and(frame, mask) to retain only<br>the hand and face regions.<br>11. Optionally, detect specific hand gestures by calculating the distances between key<br>hand landmarks (e.g., thumb tip to index finger tip) and comparing them to predefined<br>thresholds.<br>12. Display the processed frame with the background removed using cv2.imshow().<br>13. If gesture detection is enabled, overlay the recognized gesture label (e.g., "Open<br>Hand", "Fist") on the processed frame.<br>14. Continue processing frames until the user presses a specific key (e.g., 'q') to terminate<br>the loop.</p><h3>Links</h3><p>[No links found]</p><h3>Tables</h3><p>[No tables found]</p><h3>Images</h3><p>[No images found]</p></div><div class="page-block"><h2>Page 5</h2><h3>Text</h3><p>15. Release the video capture object and close any OpenCV windows using cap.release()<br>and cv2.destroyAllWindows().<br>--------------------------------------------------------------------------------------------------------------------------<br>2.3 Real-Time Translation to Text and Audio<br>Once the gestures are recognized and interpreted, they are translated into text using a<br>sequence-to-sequence model. The text is then passed through a text-to-speech (TTS) engine,<br>such as Google Text-to-Speech or Amazon Polly, which synthesizes the text into natural-<br>sounding speech.<br>The integration of these components ensures that the system operates in real-time, with<br>minimal delay between gesture recognition and speech output, facilitating smooth<br>communication in live audio calls. The TTS engine also allows for customization of speech<br>parameters (pitch, speed, tone) to suit the user's preferences and ensure clarity.<br>2.4 Output Module: Text-to-Speech (TTS) Conversion<br>The Output Module is responsible for converting the recognized text into spoken audio using<br>TTS engine. A high-quality TTS engine, such as Google Text-to-Speech, Microsoft Azure TTS, or<br>Amazon Polly, is used to synthesize the text into natural-sounding speech. The system allows<br>for customization of the speech output, adjusting parameters like pitch, speed, and tone to<br>ensure that the audio is clear and easily understood by the listener. The TTS engine is<br>optimized for real-time performance, ensuring that the audio is generated with minimal<br>latency. This low-latency capability is crucial for maintaining a smooth, conversational flow,<br>with the system aiming to produce speech without significant delays. The audio output is then<br>transmitted to the other participant in the conversation, enabling real-time voice<br>communication.<br>2.5 System Integration and Real-Time Communication<br>To facilitate real-time communication, the system was integrated with existing voice call<br>platforms like WebRTC, Twilio, or other SIP-based systems. The integration ensures that the<br>system can handle continuous data streams, processing video input and converting it to audio<br>in real time. The system works as follows:<br> The camera feed captures the user's gestures and sends them to the Processing Unit.<br> The recognized gestures are translated into text, which is then sent to the Output Mod-<br>ule for speech synthesis.<br> Finally, the TTS-generated speech is transmitted to the other participant in the call,<br>enabling seamless interaction.</p><h3>Links</h3><p>[No links found]</p><h3>Tables</h3><p>[No tables found]</p><h3>Images</h3><p>[No images found]</p></div><div class="page-block"><h2>Page 6</h2><h3>Text</h3><p>In addition to the technical aspects, the system features a user-friendly interface, which<br>provides feedback to the user. This might include visual indicators of recognized gestures or<br>text, giving users confidence that their input is being accurately understood.<br>3. Results:<br>The observed performance metrics are mentioned, key trends highlighted, and relevant<br>conclusions drawn from experimental analyses.<br>3.1. System Evaluation<br>The system is evaluated, as for different real-time and environmental conditions within which<br>standard sign languages were recognized;<br>3.1.1. Experimental Setup<br>This evaluation involved:<br> 150 images taken in different lighting and background and with different positions of<br>hand.<br> 12 single-handed gestures performed with both hands, each recorded in 150 RGB<br>frames.<br> Recording all diverse hand gestures at various distances to evaluate depth and hand<br>pose variations.<br>3.1.2. Recognition Accuracy and Performance<br>The system's accuracy, as presented in Table 1, demonstrates its effectiveness:<br>1) This means that the system recognizes gestures with pure accuracy under standard<br>lighting conditions: 98.2 percent.<br>2) It carried it further to see under low-light conditions, with a successful presentation<br>from the system, which maintained an accuracy figure of about 95.6 percent.<br>3.1.3. Precision, Recall, and F1 Score<br>In evaluating the system's effectiveness, three performance measurements of particular<br>importance were investigated:<br>1) Precision – Measures the proportion of correctly identified signs among all signs<br>predicted.<br> 94% precision means that 94% of signs identified were indeed correct.<br> This means that there would only be a few false positives, preventing a wrong<br>interpretation of some non-gesture movements as valid signs.<br>2) Recall – Measures the proportion of correctly identified signs among all actual signs.<br> Notably, the system has 91% recall; that is, 91% of signs actually performed were<br>detected.</p><h3>Links</h3><p>[No links found]</p><h3>Tables</h3><p>[No tables found]</p><h3>Images</h3><p>[No images found]</p></div><div class="page-block"><h2>Page 7</h2><h3>Text</h3><p> This means it covers a wide range of gestures, thus generating false alarms<br>sometimes.<br>3) F1 Score – Provides a balanced measure of precision and recall.<br> The system scored an F1 Score of 0.92, indicating very high performance.<br> A balance such as this is critical in actual real-time recognition of sign language in<br>the precision of detecting and processing the gestures.<br>The results confirm that the system effectively recognizes gestures with high accuracy,<br>minimal errors, and robustness across diverse environments, making it a reliable tool for<br>real-time sign language translation.<br>3.2. Figures, Tables and Schemes<br>Figure 2. Illustrates the sign recognition results. The study included 12 single-handed gestures,<br>performed with both arms, each captured in 150 RGB frames.<br>Table 1. Performance Across Different Test Conditions<br>Test Condi(cid:415)on Accuracy (%) Latency (per frame)<br>Standard Ligh(cid:415)ng 98.2 1 ms<br>Low Ligh(cid:415)ng 95.6 1 ms<br>Mul(cid:415)ple Sign<br>92.5 1 ms<br>Languages</p><h3>Links</h3><p>[No links found]</p><h3>Tables</h3><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Test Condi(cid:415)on</th>
      <th>Accuracy (%)</th>
      <th>Latency (per frame)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Standard Ligh(cid:415)ng</td>
      <td>98.2</td>
      <td>1 ms</td>
    </tr>
    <tr>
      <td>Low Ligh(cid:415)ng</td>
      <td>95.6</td>
      <td>1 ms</td>
    </tr>
    <tr>
      <td>Mul(cid:415)ple Sign\nLanguages</td>
      <td>92.5</td>
      <td>1 ms</td>
    </tr>
  </tbody>
</table><h3>Images</h3><img src="extracted_images\page7_0.png" alt="Image from Page 7"><img src="extracted_images\page7_1.png" alt="Image from Page 7"></div><div class="page-block"><h2>Page 8</h2><h3>Text</h3><p>Table 2. Precision, Recall, and F1 Score for Recognition and Classification<br>Types Precision Recall F1 Score<br>Recogni(cid:415)on 0.94 0.91 0.92<br>Classifica(cid:415)on 0.95 0.90 0.92<br>4. Discussion<br>Our evaluation shows that the performance of this system is exceptional in recognizing sign<br>language in different conditions. It has achieved an accuracy of 98.2% under standard lighting<br>and remains highly efficient in low-light conditions with 95.6% accuracy, demonstrating its<br>versatility.<br>The system also managed to perform well with multiple sign languages at 92.5% accuracy for<br>real-world application. The recognition precision score was 94% while classification had a<br>score of 95% and recall was 91% and 90%, respectively. Therefore, it minimizes errors while<br>maintaining a good balance of detecting gestures and correctly identifying them. The F1 Score<br>stood at 0.92, which certifies its reliability and with a latency of just 1 ms per frame, it makes<br>it a perfect candidate for real-time applications. These results were consistent with previous<br>work on the subject and strengthens the idea of the system being an effective tool for sign<br>language communication.<br>Beyond accuracy, this system could also have a major impact on assistive technology for<br>education and communication in real-time for the deaf and mute community. An<br>improvement roadmap could involve recognizing more complex gestures enhancing its<br>efficiency for mobile and wearable devices. Moreover, new techniques, such as self-<br>supervised learning or transformer-based models, could improve the system through an<br>expanded dataset. Finally, suggestions from actual sign language users will play a key role in<br>fine-tuning the system so that it meets real-world demands in a tangible manner.<br>5. Conclusion<br>The AI/ML-based real-time sign language converter offers a novel solution to the<br>communication challenges faced by deaf and mute individuals. By leveraging AI and ML<br>technologies, the system enables seamless communication via audio calls, breaking down<br>language barriers and promoting inclusivity. Experimental results demonstrate its high<br>accuracy, low latency, and robust performance, making it a valuable tool for real-time<br>communication in various domains. The present model can be extended to supporting deaf<br>students by providing real-time translation during lectures. Also, facilitating communication<br>between healthcare providers and patients who are deaf or mute.</p><h3>Links</h3><p>[No links found]</p><h3>Tables</h3><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Types</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Recogni(cid:415)on</td>
      <td>0.94</td>
      <td>0.91</td>
      <td>0.92</td>
    </tr>
    <tr>
      <td>Classifica(cid:415)on</td>
      <td>0.95</td>
      <td>0.90</td>
      <td>0.92</td>
    </tr>
  </tbody>
</table><h3>Images</h3><p>[No images found]</p></div><div class="page-block"><h2>Page 9</h2><h3>Text</h3><p>Appendix A: Libraries and Tools Used<br>This appendix gives the details of the main libraries and tools that have been implemented<br>within the real-time hand and face detection algorithm.<br>A.1. OpenCV (Open Source Computer Vision Library)<br>Design definition:<br>OpenCV is an open-source computer vision and machine learning library, which provides a<br>common infrastructure for the real-time computer vision applications.<br>Purpose in the Project:<br> To capture and process the video frames.<br> To convert images between different colour spaces (BGR to RGB for example).<br> To perform bitwise operations for background removal.<br> To process the frames and display using cv2.imshow().<br> To manage the video input/output using cv2.VideoCapture() and cv2.VideoWriter().<br>A.2. MediaPipe<br>Definition:<br>It is a cross-platform framework developed by Google to create machine learning pipelines<br>optimized for real-time vision based tasks.<br>Purpose in the Project:<br> Detecting hand landmarks using mp.solutions.hands.Hands().<br> Detecting face features using mp.solutions.face_detection.FaceDetection().<br> Efficient processing of video frames using built-in models meant for low latency.<br>A.3. NumPy (Numerical Python)<br>Definition:<br>NumPy is a core package for numerical computation in Python. It provides support for large<br>multidimensional arrays and matrices, as well as a library of high-level mathematical functions<br>to operate on these arrays.<br>Purpose in Project:<br> Creating and manipulating image masks using np.zeros_like(frame).<br> Doing array operations i.e. manipulating arrays to efficiently process the image.<br> Optimization in vectorized computation instead of loops gives highly efficient<br>performance.</p><h3>Links</h3><p>[No links found]</p><h3>Tables</h3><p>[No tables found]</p><h3>Images</h3><p>[No images found]</p></div><div class="page-block"><h2>Page 10</h2><h3>Text</h3><p>Appendix B: Further Observations and Considerations<br>B.1. Environmental Factors that Influence Recognition Performance<br>Although the system performed very well under different conditions, the following<br>environmental factors affected its accuracy.<br> Lighting Conditions: while standard lighting is associated with an accuracy of 98.2%,<br>reduced lighting causes the value to fall to about 95.6%.<br> Background Complexity: Cluttered backgrounds introduced noise, which makes hand<br>segmentation much more difficult.<br> Camera Distance and Angle: Gestures done far from the camera or at really extreme<br>angles did not lead to correct but slight misclassifications.<br>B.2. Real Time Performance with Latency Considerations<br> The system provides a constant frame latency of 1ms and making it suitable for real-<br>time applications.<br> Computational efficiency was achieved by leveraging GPU-optimized MediaPipe<br>models.<br>B.3. Future Enhancements<br> Integrating depth sensors for more accurate segmentation of hands in complex<br>backgrounds.<br> Using Transformer-based models for more robust sign language recognition across<br>different languages.<br> Improving the data collection to include extensive variations of shape of hands, skin<br>tones, and environments.<br>References:<br>1. Liu, Y., Wu, J., & Zhang, T. (2020). "Real-time Sign Language Recognition with Multi-<br>modal Deep Learning." IEEE Transactions on Neural Networks.<br>2. Rahmani, H., & Bhowmick, D. (2018). "Contextualized Deep Learning for Sign Language<br>Recognition." Journal of AI and Computer Vision.<br>3. Hussain, A., & Ahmed, A. (2018). "Deep Learning Approaches to Cross-Linguistic Sign<br>Language Recognition." IEEE Access.<br>4. Google AI Blog (2020), "AI for Sign Language: Google's Machine Learning Break-<br>through."<br>5. SignAll (2020). "Real-Time Sign Language Recognition Using Multi-Camera System."<br>SignAll Technologies.</p><h3>Links</h3><p>[No links found]</p><h3>Tables</h3><p>[No tables found]</p><h3>Images</h3><p>[No images found]</p></div><div class="page-block"><h2>Page 11</h2><h3>Text</h3><p>6. Mubarak, M. S., & Pandey, R. (2020). "Multi-Modal Sign Language Recognition using<br>Computer Vision and AI." Journal of Assistive Technologies.<br>7. Jia, X., Li, Z., & Chen, X. (2021). "Edge Computing for Real-Time Sign Language Recog-<br>nition." IEEE Internet of Things Journal.<br>8. IEEE Transactions on Neural Networks (2018). "Real-Time Sign Language Recognition<br>Using Convolutional Neural Networks."<br>9. IEEE Transactions on Human-Machine Systems (2020). "A Survey of Sign Language<br>Recognition and Translation Systems."<br>10. IEEE Sensors Journal (2019). "Gesture Recognition Using Sensor Gloves for Sign Lan-<br>guage Interpretation."</p><h3>Links</h3><p>[No links found]</p><h3>Tables</h3><p>[No tables found]</p><h3>Images</h3><p>[No images found]</p></div></body></html>